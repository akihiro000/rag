from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import chromadb
from chromadb.config import Settings
import torch

# Chromaã‹ã‚‰ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã‚€
client = chromadb.PersistentClient(path="vectorstore/chroma_db")
collection = client.get_or_create_collection(name="pdf_chunks")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ãƒ»LLMï¼‰
retriever = SentenceTransformer("all-MiniLM-L6-v2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

# å¯¾è©±ãƒ«ãƒ¼ãƒ—
while True:
    query = input("ğŸ” è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    if query.lower() in ["exit", "quit"]:
        break

    # è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦é¡ä¼¼æ–‡æ¤œç´¢
    q_emb = retriever.encode([query]).tolist()[0]
    results = collection.query(query_embeddings=[q_emb], n_results=3)

    # æ¤œç´¢çµæœã‚’çµåˆã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŒ–
    context = "\n\n".join(results["documents"][0])

    prompt = f"""
ä»¥ä¸‹ã®æ–‡ç« ã«åŸºã¥ã„ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡ç« :
{context}

è³ªå•:
{query}

ç­”ãˆ:
"""

    # LLMã§å›ç­”ç”Ÿæˆ
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
        )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("\nğŸ§  å›ç­”:\n", answer)
