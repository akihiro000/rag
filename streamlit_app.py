import streamlit as st
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import chromadb
import torch

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
retriever = SentenceTransformer("all-MiniLM-L6-v2")
tokenizer = AutoTokenizer.from_pretrained("rinna/japanese-gpt2-medium", use_fast=False)
model = AutoModelForCausalLM.from_pretrained("rinna/japanese-gpt2-medium")

client = chromadb.PersistentClient(path="vectorstore/chroma_db")
collection = client.get_or_create_collection(name="pdf_chunks")

# UIè¨­å®š
st.set_page_config(page_title="PDFè³ªå•AI", page_icon="ğŸ“„")
st.title("ğŸ“„ PDFã‹ã‚‰ç­”ãˆã‚‹AI")
st.markdown("PDFã«åŸºã¥ã„ã¦ã€è³ªå•ã«ç­”ãˆã¾ã™ã€‚")

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if query:
    with st.spinner("æ¤œç´¢ã¨ç”Ÿæˆä¸­..."):
        q_emb = retriever.encode([query]).tolist()[0]
        results = collection.query(query_embeddings=[q_emb], n_results=3)
        context = "\n\n".join(results["documents"][0])

        prompt = f"""
        ä»¥ä¸‹ã®æ–‡ç« ã‚’å‚è€ƒã«ã—ã¦ã€æ¬¡ã®è³ªå•ã«å¯¾ã™ã‚‹æ—¥æœ¬èªã®ç­”ãˆã‚’ç°¡æ½”ã«1ã¤æ›¸ã„ã¦ãã ã•ã„ã€‚
        
        æ–‡ç« :
        {context}
        
        è³ªå•: {query}
        """


        inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.5,
                top_k=30,
                top_p=0.85,
                repetition_penalty=1.2
            )

        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        st.markdown("### ğŸ§  å›ç­”")
        st.text_area("ğŸ§  å›ç­”", value=answer.replace(prompt, "").strip(), height=200)

